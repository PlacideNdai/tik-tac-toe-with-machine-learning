Intro Data Science
Lab 4: Unsupervised Collective Learning System with Tic Tac Toe


In this report I reflect on my observations of my program's behavior (after verifying that it works correctly). I tested the program's behavior and learning ability on a few different values of betaReward and betaPunish.

Over the course of several hours I tried five different combinations of beta values and recorded my observations somewhat extensively. Here is a general ranking of how well I believe the machine was able to learn with each set of beta values:

1. Different beta values   R = 0.5   P = 0.25
2. Early observations      R = 0.5   P = 0.5
3. Reward / Inaction       R = 0.5   P = 0
4. Lower beta values       R = 0.25  P = 0.25
5. Higher beta values      R = 0.75  P = 0.75

More in-depth observations from each trial follow.


* Early observations

betaReward = 0.5
betaPunish = 0.5

As soon as I began playing against the AI, I decided to always play intelligently instead of going easy on it. As a result, I won each of the first ~5 games easily. At some point the machine made a series of seemingly very lucky decisions in a row, in which it blocked each of my attempts to win and was rewarded for the first time.

After this the machine's performance improved dramatically; soon it was playing so well every time that I couldn't beat it. It always moved in the center first, and the best I could do was force a tie by making my first move in a corner and following the known TTT strategy from there. At some point I decided to see what would happen if I unintelligently made my first move in the side, but played intelligently from there: the machine won.

I was impressed with how good the machine had become so quickly, and looked through the STM to observe the moves it had learned to make, and to avoid. I could see that it had solidly learned to block my win in a few different situations. While this was impressive, I couldn't help but notice that there were still many, many more situations that the machine had never encountered and hadn't learned anything about.

I came up with an idea to beat my trained AI. I started a new game, and the machine began by moving in the center and then a corner. But instead of blocking the opposite corner, I ignored it and made my own two-in-a-row elsewhere. The machine didn't know it was poised to win! Instead of completing its free victory, it moved somewhere else at random, allowing me to complete my three-in-a-row. It didn't know how to win, or even to block me, because I had never let it encounter a situation like that before. It turned out the secret to beating the seemingly unbeatable AI was to play UNintelligently.


* Lower beta values

betaReward = 0.25
betaPunish = 0.25

With beta values this low, training the machine was a bit more frustrating. It took it 17 games to tie me once. Then it took it 11 more to tie me again. Then it tied me a couple more times immediately after that.

All of these tie games happened when the machine moved first in the center, but it still kept trying other starting moves sometimes anyway. At one point it accidentally bamboozled me in a game where it started in the corner, and all of a sudden the STM showed the corner first move as better than center first. (I did also punish it many times when it moved first in the center, because it made a lot of brainless moves during most of those games.)

At this point I decided to move on, because the machine was learning so slowly and progress was easily lost. The tedium of unsupervised learning was very noticeable in this case: all moves of each game had to be taken as a whole; they were either all bad, or all good. The machine couldn't know it had been right to play first in the center, or block my first attempt at a win, if I ended up beating it anyway. It occurred to me that I could try to teach it more effectively for how the algorithm was set up to learn-- I could let it win sometimes when it made a good move. But that would be supervising the learning, and I decided it would better fit the point of the exercise to just try to beat it every time and see if it could figure out how to get me back.


* Higher beta values

betaReward = 0.75
betaPunish = 0.75

Since I can easily beat the untrained AI, it tried wildly different things the first few games, until it accidentally forced me into a tie in a game where it started by playing in the side. One look at the STM and I knew it was unlikely to ever try anything else again.

At least, that's what I thought-- but when I played slightly differently and beat it the next game, the STM values flipped dramatically in favor of a center first move. I learned to never assume I know what an algorithm will do with such high beta values.

After a few more games, a pattern emerged: that the machine consistently avoided any hint of a pattern. Each time I beat the machine, it changed its strategy entirely. It cycled through the three possible starting moves almost like clockwork. It never lingered on one type of game for long enough to learn anything about it, because the punishment for losing was so severe.

I decided to stop when it was clear that these beta values were just as ineffective as the low ones, and when I started to imagine the machine begging me to stop because it had tried everything and was still being punished relentlessly.


* Reward / Inaction

betaReward = 0.5
betaPunish = 0

I decided to try giving the machine a break from being punished. The first few games looked pretty hopeless: the program learned nothing when I beat it, so it would blindly make random moves until it would stumble into a rewarding situation.

This happened sooner than I expected; the machine accidentally bamboozled me on a game where it started by moving in the side again (I'm starting to wonder if making the first move in the side isn't as defective of a TTT strategy as I thought). (That was a joke. I know it is. But playing against a machine that moves randomly is different; it causes you to take risks you would never take when playing against a human.)

The machine started experimenting with different variations of this game. It had nothing to lose when it didn't work, but its resolve that this was the way to win or tie was reinforced each time it managed to do so.

It had seemed like a promising premise that the machine could experiment with different strategies and not be punished for doing so, but once the side-first strategy caught on, I realized there was no way to teach it anything else. Even if I were to play wildly different moves and confuse the machine into losing, over and over again, it wouldn't matter, because it would never unlearn what it had learned without first overcoming 99-to-1 odds in the STM.

This AI was scarily relentless with the one strategy it knew had the potential to work. I could still beat it by moving in unexpected places and exposing the machine to situations it hadn't seen, but I realized that would only last so long. Eventually it would learn to cope with every variation of the side-first game, and there was no stopping it.

Still, if I had had the patience to keep playing that side-first game over and over, it would have taken a long time, and many shots in the dark, for it to actually learn every variation of it. It certainly did learn how to win every time when I played as intelligently as I could, and would never unlearn that if I didn't reset the STM, but its learning overall was inflexible, and its capacity to learn new ways of winning was severely handicapped because it was almost never rewarded for its myriad experiments.

It does seem likely that rewarding the machine is indeed more effective for learning than punishing it the same amount, but I think there is still very real value in the ability to learn from failures as well as successes.


* Different beta values

betaReward = 0.5
betaPunish = 0.25

This version of the machine's behavior was much closer to that of the original 0.5-0.5. After ~10 games it started being rewarded and sinking thoroughly into the center-first strategy (a different variation of it than before though).

I wondered if I could make it worse at winning in this way by tricking it a few times. I did. (By now I had learned the general strategy of moving in weird ways to put the machine in situations it didn't know anything about; I could pretty much always use this to beat it even when it learned some particular strategy really well.)

Even so, the machine started most games in the center, and quickly learned many variations of the center-first game, so that soon I could almost never win. Sometimes I could, if I played crazily *and* got lucky, but if I played with normal strategy, it would reliably beat me (not just tie, but actually win).

This was infuriating. And it seems to be the most effective learning AI I created-- with nonzero punishment, but higher reward, but not *too* high. Even when I tricked the machine into losing several times in a row, it was not discouraged, but it was still able to get better in response to the losses. This result doesn't surprise me; it follows from what we discussed in class: that reward is more effective than punishment, but it's best to use both in moderation.


* Retrospective

Playing TTT against this AI was a strange and intriguing experience for me. I always thought TTT was a very simple game, and that once you learn the basic strategy there's no way to lose (and, often, no way to win because your opponent usually knows the strategy too). But when playing against a CLS, the strategy changes. It no longer makes sense to block the opponent every time it makes two in a row, because you usually have a better chance of winning by taking the risk and leaving it open. At some point when I was playing the last variant, I realized *I* was learning how to play against it specifically-- I was figuring out which moves it knew how to respond to and which ones could still confuse it. It became a battle of wits as the machine learned how to win in more and more situations and I had to keep thinking of new ways to introduce situations it hadn't seen yet.

At some point I realized that the only way to stop the machine from becoming completely unbeatable and inevitably taking over the world was to stop training it.

